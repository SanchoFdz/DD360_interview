{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Intento de MLOPS\n",
        "\n",
        "Estamos en pleno HotSale en la empresa en la que trabajo así que no tuve mucho tiempo para debuggear esto y documentarlo, pero dejo la estructura, aunque no corre (pero por pequeños detalles) "
      ],
      "metadata": {
        "id": "9yksDiZKelm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Instalaciones\n",
        "!pip install unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSQc1CALVnbt",
        "outputId": "0dda48ac-0759-4a58-d338-6afad584b6dc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "01ltXl5qVMYR"
      },
      "outputs": [],
      "source": [
        "#Paquetería\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import unidecode\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "from joblib import load\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Drops"
      ],
      "metadata": {
        "id": "lTI4cMKgVzd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def first_drop(raw_data):\n",
        "  raw_data.drop(['subtitle','timestamp',\n",
        "                'price_currency','age_in_years', 'department_type','orientation',\n",
        "                'floor_situated','disposition','main_name','link', 'price', 'price_mod', 'price_square_meter',\n",
        "                'since_period', 'since_value', 'attributes','id'], axis = 1,\n",
        "                inplace = True)\n",
        "  \n",
        "  return raw_data"
      ],
      "metadata": {
        "id": "QtBpvxSvVY-A"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Limpieza de Texto"
      ],
      "metadata": {
        "id": "IBBufZLTVyA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def addressPreprocessing(raw_data, umbral=7):\n",
        "  # Más facil manejar todo en minusculas, sin acentos y controlando los espacios, al no ser\n",
        "  # campos uniformes, tenemos una columna muy sucia, parece que cada quien que publica su \n",
        "  # departamento debe ingresar a mano la direccion.\n",
        "  raw_data['location'] = (raw_data['location'].apply(lambda x: unidecode. #unidecode sirve para limpiar los acentos\n",
        "                                                    unidecode(x.lower(). #con lower nos hacemos la vida más fácil al trabajar en minusculas\n",
        "                                                              strip()))) #con strip controlamos los espacios antes y después\n",
        "\n",
        "  raw_data['colonia'] = (raw_data['location']\n",
        "                        .apply(lambda x: \n",
        "                                re.search(',(.*?),', x).group(1).strip() # quédate con TODO (incluyendo espacios)entre las primeras dos comas\n",
        "                                if re.search(',(.*?),', x) #checa que exista algo entre las dos primeras comas\n",
        "                                else np.nan)) #si no, nan y nos clavamos bien\n",
        "\n",
        "  raw_data['Estado'] = (raw_data['location']\n",
        "                        .apply(lambda x: re.#utilizamos regex\n",
        "                              search('(distrito federal|distrito federal|nuevo leon|nuevo león|baja california|distrito federal)', #sustituye por esto\n",
        "                                      x).group() if re\n",
        "                              .search('(distrito federal|cdmx|nuevo leon|nuevo leon|baja california|ciudad de mexico)', #busca estas formas de escribir el estaod\n",
        "                                      x) else np.nan)) #si no está esa forma de escribir el estado, nos pone NaN y hacemos deep dive\n",
        "\n",
        "  colonias = { # Todas las colonias que tenemos en el dataset (limitacion: es poco escalable)\n",
        "      \"\\\\bcondesa\\\\b\":\"condesa\",\n",
        "      \"\\\\broma, monterrey\\\\b\": \"roma\",\n",
        "      \"\\\\bprivada roma\\\\b\": \"roma\",\n",
        "      \"\\\\broma sur, monterrey\\\\b\": \"roma\",\n",
        "      \"\\\\broma privada\\\\b\":\"roma\",\n",
        "      \"\\\\broma norte\\\\b\": \"roma norte\",\n",
        "      \"\\\\broma sur\\\\b\": \"roma_sur\",\n",
        "      \"\\\\bqueretaro\\\\b\": \"queretaro\",\n",
        "      \"\\\\bhermosillo\\\\b\": \"hermosillo\",\n",
        "      \"\\\\bcuauhtemoc\\\\b\": None,\n",
        "      \"\\\\bnarvarte\\\\b\": \"narvarte\",\n",
        "      \"\\\\bdel valle\\\\b\": \"del valle\",\n",
        "      \"\\\\bbaja california\\\\b\":\"baja california\",\n",
        "      \"\\\\bbenito juarez\\\\b\":None\n",
        "  }\n",
        "\n",
        "  raw_data['colonia'] = raw_data['location']\n",
        "\n",
        "  for old_word, new_word in colonias.items(): # Buscamos si la colonia que identificamos existe en la direccion\n",
        "      mask = raw_data['colonia'].str.contains(old_word, case=False, na=False)\n",
        "      raw_data.loc[mask, 'colonia'] = new_word\n",
        "\n",
        "  value_counts = raw_data['colonia'].value_counts()\n",
        "\n",
        "  to_replace = value_counts[value_counts < umbral].index # Cambiamos aquellas colonias que aparecen menos que un umbral por \"otra\"\n",
        "\n",
        "  raw_data.loc[raw_data['colonia'].isin(to_replace), 'colonia'] = 'otra'\n",
        "\n",
        "  raw_data.loc[raw_data['Estado'] == 'baja california', 'colonia'] = 'baja california'\n",
        "\n",
        "  encoder = OrdinalEncoder() # Nuestra columna de estados es una columna categórica\n",
        "  raw_data['Estado_encoded'] = encoder.fit_transform(raw_data[['Estado']])\n",
        "\n",
        "  #Nuestro problem domain y dataset nos da 5 clústers en los que podría caer\n",
        "  kmeans = KMeans(n_clusters=5) #Las 5 categorías que tenemos: baja california, roma nuevo leon, roma N, roma S, Otra\n",
        "  labels = kmeans.fit_predict(raw_data[['lat', 'lon', 'Estado_encoded', 'final_price']])\n",
        "\n",
        "  raw_data['Colonia_KMeans'] = labels\n",
        "\n",
        "  #Agregamos la moda como etiqueta de Colonia\n",
        "  label_colonia = {}\n",
        "  for label in set(labels):\n",
        "      colonia = raw_data.loc[raw_data['Colonia_KMeans'] == label, 'colonia'].mode().iloc[0] # Hacemos la imputación de la colonia por la moda (al ser categorico)\n",
        "      label_colonia[label] = colonia\n",
        "\n",
        "  raw_data.loc[raw_data['colonia'].isnull(), 'colonia'] = (raw_data\n",
        "                                                          .loc[raw_data['colonia'].isnull(),\n",
        "                                                                'Colonia_KMeans'].map(label_colonia))\n",
        "  \n",
        "  return raw_data"
      ],
      "metadata": {
        "id": "ZlLKpKXQVvYP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Drop 2"
      ],
      "metadata": {
        "id": "t7LyCR6YWL50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def second_drop(raw_data):\n",
        "  raw_data.drop(['location', 'address',\n",
        "                'since','Estado_encoded', 'Colonia_KMeans', 'num_floors', 'monthly_fee',\n",
        "                'apartments_per_floor', 'description', 'cellars'], \n",
        "                axis = 1, inplace = True)\n",
        "  \n",
        "  raw_data.dropna(inplace=True)\n",
        "\n",
        "  return raw_data"
      ],
      "metadata": {
        "id": "Yo5Gk4rjWNF1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoding\n"
      ],
      "metadata": {
        "id": "qyMXAqZvWfZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encoding(raw_data):\n",
        "  colonia_encoded = pd.get_dummies(raw_data['colonia'])\n",
        "  estado_econded = pd.get_dummies(raw_data['Estado'])\n",
        "\n",
        "  clean_data = pd.concat([colonia_encoded, estado_econded, raw_data], axis=1)\n",
        "  clean_data.drop(['colonia', 'Estado', 'vendor'], inplace = True, axis=1)\n",
        "\n",
        "  return clean_data"
      ],
      "metadata": {
        "id": "cz4_MXxxWhqz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing / Splitting"
      ],
      "metadata": {
        "id": "EG8nwXhPWmH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_scale(clean_data, test_size=0.2):\n",
        "  X = clean_data.drop('final_price', axis=1)\n",
        "  y = clean_data['final_price']\n",
        "\n",
        "  x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
        "\n",
        "  scaler = StandardScaler()\n",
        "\n",
        "  scaler.fit(x_train)\n",
        "\n",
        "  x_train = scaler.transform(x_train)\n",
        "  x_test = scaler.transform(x_test)\n",
        "\n",
        "  return x_train, x_test, y_train, y_test"
      ],
      "metadata": {
        "id": "PDAwshkcWqVj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models, metrics and results"
      ],
      "metadata": {
        "id": "C0RQpFsiWy9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MAPE(y_test, y_pred): \n",
        "    y_test, y_pred = np.array(y_test), np.array(y_pred)\n",
        "    return np.mean(np.abs((y_test - y_pred) / y_test)) * 100"
      ],
      "metadata": {
        "id": "4wP1N63EW063"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def xGBoost_PriceModel(x_train, y_train, x_test = None, y_test = None, feature_ranking = True, path_to_weights = None):\n",
        "  xg = GradientBoostingRegressor(learning_rate= 0.1, n_estimators= 200)\n",
        "  xg.fit(x_train, y_train)\n",
        "\n",
        "  if x_test and y_test:\n",
        "    y_pred = xg.predict(x_test)\n",
        "    print(f\"xGBoost RMSE: {mean_squared_error(y_test, y_pred, squared=False)}\")\n",
        "    print(f\"R2: {xGb.score(x_test,y_test)}\")\n",
        "    print(f'MAPE: {MAPE(y_test,y_pred)}')\n",
        "\n",
        "  if feature_ranking:\n",
        "    importances = xg.feature_importances_\n",
        "\n",
        "    feature_names = X.columns\n",
        "\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': importances\n",
        "    })\n",
        "\n",
        "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "    importance_df = importance_df.reset_index(drop=True)\n",
        "\n",
        "    print(\"Feature ranking:\")\n",
        "    for i in range(importance_df.shape[0]):\n",
        "        print(f\"Feature {importance_df.loc[i, 'Feature']}, Importance: {importance_df.loc[i, 'Importance']}\")\n",
        "\n",
        "    if path_to_weights:\n",
        "      dump(xg, os.path.join(path_to_weights,'xg_model.joblib'))\n",
        "\n",
        "    return xg"
      ],
      "metadata": {
        "id": "9fbxjfFwX45S"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rFBoost_PriceModel(x_train, y_train, x_test = None, y_test = None, feature_ranking = True, path_to_weights = None):\n",
        "  rf = RandomForestRegressor(max_depth= None, min_samples_leaf= 1, n_estimators= 150)\n",
        "  rf.fit(x_train, y_train)\n",
        "\n",
        "  if x_test and y_test:\n",
        "    y_pred = rf.predict(x_test)\n",
        "    print(f\"Random Forest RMSE: {mean_squared_error(y_test, y_pred, squared=False)}\")\n",
        "    print(f\"R2: {forest_model.score(x_test,y_test)}\")\n",
        "    print(f'MAPE: {MAPE(y_test,y_pred)}')\n",
        "\n",
        "  if feature_ranking:\n",
        "    importances = rf.feature_importances_\n",
        "\n",
        "    feature_names = X.columns\n",
        "\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': importances\n",
        "    })\n",
        "\n",
        "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "    importance_df = importance_df.reset_index(drop=True)\n",
        "\n",
        "    print(\"Feature ranking:\")\n",
        "    for i in range(importance_df.shape[0]):\n",
        "        print(f\"Feature {importance_df.loc[i, 'Feature']}, Importance: {importance_df.loc[i, 'Importance']}\")\n",
        "\n",
        "  if path_to_weights:\n",
        "    dump(rf, os.path.join(path_to_weights,'rf_model.joblib'))\n",
        "\n",
        "  return rf"
      ],
      "metadata": {
        "id": "ZMLL8e5rYFMk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predictPrice(raw_data, model = 'xg', price='square_meter', umbral = 7,  test_size=0.2, path_to_weights=None):\n",
        "  m2 = raw_data['m2']\n",
        "\n",
        "  raw_data = first_drop(raw_data)\n",
        "\n",
        "  raw_data = addressPreprocessing(raw_data, umbral)\n",
        "\n",
        "  raw_data = second_drop(raw_data)\n",
        "\n",
        "  clean_data = encoding(raw_data)\n",
        "\n",
        "  if model == 'xg':\n",
        "    if path_to_weights:\n",
        "      model = load(path_to_weights)\n",
        "    else:\n",
        "      x_train, y_train,_,_ = split_scale(clean_data, test_size)\n",
        "      model = xGBoost_PriceModel(x_train, y_train, x_test = None, y_test = None, feature_ranking = True)\n",
        "  else:\n",
        "    if path_to_weights:\n",
        "      model = load(path_to_weights)\n",
        "    else:\n",
        "      x_train, y_train,_,_ = split_scale(clean_data, test_size)\n",
        "      model = rFBoost_PriceModel(x_train, y_train, x_test = None, y_test = None, feature_ranking = True)\n",
        "  \n",
        "  X = clean_data.drop('final_price', axis=1)\n",
        "  y_pred = model.predict(X)\n",
        "\n",
        "  if price == \"square_meter\":\n",
        "    return y_pred/m2\n",
        "  else:\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "n9UuQmL5asdx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Carga de archivo\n",
        "# Correr la siguiente instruccion solo si lo planean correr en Colab\n",
        "#os.chdir(\"/content\")\n",
        "raw_data = pd.read_csv(\"./reto_precios.csv\")"
      ],
      "metadata": {
        "id": "WTJUt6DTVVrD"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = predictPrice(raw_data, model = 'xg', price='square_meter', umbral = 7,  test_size=0.2, path_to_weights=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "XplSa54wdoLS",
        "outputId": "d62688cd-e72b-45e3-ff66-b699e7bf3910"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-6618961a436c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictPrice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'xg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'square_meter'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mumbral\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-2fd3720f3475>\u001b[0m in \u001b[0;36mpredictPrice\u001b[0;34m(raw_data, model, price, umbral, test_size, path_to_weights)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m       \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxGBoost_PriceModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_ranking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpath_to_weights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-53561387bc8e>\u001b[0m in \u001b[0;36mxGBoost_PriceModel\u001b[0;34m(x_train, y_train, x_test, y_test, feature_ranking, path_to_weights)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mxGBoost_PriceModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_ranking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mxg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradientBoostingRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mxg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;31m# trees use different types for X and y, checking them separately.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         X, y = self._validate_data(\n\u001b[0m\u001b[1;32m    430\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"coo\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    582\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmulti_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_numeric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_numeric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    398\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [403, 101]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N1IGGKUId2L8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}